This is a fun project I wanted to build from scratch.

A minimal transformer inference engine built from scratch in PyTorch, focused on understanding KV caching, prefill vs decode, and inference performance.

This project prioritizes clarity and systems understanding over training or model accuracy.


AI Usage:

I wrote all of the code myself except for some of the tests in the tests/ folder. I thought the models
could write more comprehensive tests than myself and it left me more time to work on the actual inference engine.

I also used AI to help write the plot_results and bench_sweep scripts.
